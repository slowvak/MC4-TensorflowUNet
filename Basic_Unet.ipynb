{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Basic_Unet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slowvak/MC4-TensorflowUNet/blob/master/Basic_Unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-7VxpFXbhM8",
        "colab_type": "text"
      },
      "source": [
        "Cell 1 loads up all the libraries we are using. In the first lessons, we used fastAI, while in this one we use Tensorflow with Keras. Both of these are large libraries, and its designers decided to allow programmers to load only the pieces that are relevant, which is what the 'from keras.XXX import YYY' lines do--they load just the partrt of Keras (or other library) that we want. This makes the load time faster and the final running program smaller.\n",
        "\n",
        "The second half of this cell does the familiar step of loading the data sets that I have prepared, which in this case are a series of abdominal CTs as well as hand-traced masks of the Pancreas, courtesy of The Cancer Image Archive: http://TheCancerImageArchive...."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngjE5zA0F1PE",
        "colab_type": "code",
        "outputId": "fa7fff41-4dfa-4a9d-e9a4-8acda3ad9f27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "#Cell 1\n",
        "\n",
        "!pip install -q keras\n",
        "!pip install natsort\n",
        "import numpy as np\n",
        "import numpy.ma as ma\n",
        "import os\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, Dense, Dropout, Activation, Flatten, BatchNormalization, Reshape\n",
        "from keras.layers.merge import concatenate, add\n",
        "from keras.layers.core import Lambda\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
        "from keras import backend as K\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "from natsort import natsorted\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "\n",
        "!rm -rf ./MC4-TensorflowUNet\n",
        "\n",
        "!rm -rf trainimages\n",
        "!mkdir trainimages\n",
        "!rm -rf trainmasks\n",
        "!mkdir trainmasks\n",
        "\n",
        "!rm -rf validationimages\n",
        "!mkdir validationimages\n",
        "!rm -rf validationmasks\n",
        "!mkdir validationmasks\n",
        "\n",
        "!rm -rf testimages\n",
        "!mkdir testimages\n",
        "!rm -rf testmasks\n",
        "!mkdir testmasks\n",
        "\n",
        "!git clone https://github.com/slowvak/MC4-TensorflowUNet.git\n",
        "\n",
        "!unzip -q -o \"./MC4-TensorflowUNet/Pt1.zip\"\n",
        "#!unzip -q -o \"./MC4-TensorflowUNet/Pt2-019-033.zip\"\n",
        "#!unzip -q -o \"./MC4-TensorflowUNet/Pt5-064-082.zip\"\n",
        "!mv *-Mask.jpg ./trainmasks\n",
        "!mv *.jpg ./trainimages\n",
        "\n",
        "!unzip -q -o \"./MC4-TensorflowUNet/Pt2.zip\"\n",
        "!mv *-Mask.jpg ./validationmasks\n",
        "!mv *.jpg ./validationimages\n",
        "\n",
        "!unzip -q -o \"./MC4-TensorflowUNet/Pt3.zip\"\n",
        "!mv *-Mask.jpg ./testmasks\n",
        "!mv *.jpg ./testimages"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: natsort in /usr/local/lib/python3.6/dist-packages (5.5.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'MC4-TensorflowUNet'...\n",
            "remote: Enumerating objects: 20, done.\u001b[K\n",
            "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 313 (delta 7), reused 0 (delta 0), pack-reused 293\u001b[K\n",
            "Receiving objects: 100% (313/313), 405.15 MiB | 37.63 MiB/s, done.\n",
            "Resolving deltas: 100% (109/109), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rg-8caamcSTp",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiNnuEKhkS0t",
        "colab_type": "code",
        "outputId": "b892dce0-c5a3-40ef-bfc4-39fba92103e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_X = []\n",
        "train_Y = []\n",
        "val_X = []\n",
        "val_Y = []\n",
        "test_X = []\n",
        "test_Y = []\n",
        "\n",
        "\n",
        "trainlimit = 50000  # no limit\n",
        "validlimit = 40000\n",
        "testlimit = 40000\n",
        "\n",
        "im_list = natsorted(os.listdir('./trainimages'))\n",
        "for f in im_list:\n",
        "  pth = './trainimages/' + f \n",
        "  img = imageio.imread(pth)\n",
        "  if len(train_X) < trainlimit:\n",
        "    train_X.append(np.array(img))\n",
        "train_X = np.array(train_X)\n",
        "train_X = train_X/255\n",
        "train_X = train_X[..., np.newaxis]\n",
        "\n",
        "im_list = natsorted(os.listdir('./trainmasks'))\n",
        "for f in im_list:\n",
        "  pth = './trainmasks/' + f \n",
        "  img = imageio.imread(pth)\n",
        "  if len(train_Y) < trainlimit:\n",
        "    train_Y.append(np.array(img))\n",
        "train_Y = np.array(train_Y)\n",
        "train_Y = (train_Y > 0).astype(np.float32)\n",
        "train_Y = train_Y[..., np.newaxis]\n",
        "\n",
        "im_list = natsorted(os.listdir('./validationimages'))\n",
        "for f in im_list:\n",
        "  pth = './validationimages/' + f \n",
        "  img = imageio.imread(pth)\n",
        "  if len(val_X) < validlimit:\n",
        "    val_X.append(np.array(img))\n",
        "val_X = np.array(val_X)\n",
        "val_X = val_X/255\n",
        "val_X = val_X[..., np.newaxis]\n",
        "\n",
        "im_list = natsorted(os.listdir('./validationmasks'))\n",
        "for f in im_list:\n",
        "  pth = './validationmasks/' + f \n",
        "  img = imageio.imread(pth)\n",
        "  if len(val_Y) < validlimit:\n",
        "    val_Y.append(np.array(img))\n",
        "val_Y = np.array(val_Y)\n",
        "val_Y = (val_Y > 0).astype(np.float32)\n",
        "val_Y = val_Y[..., np.newaxis]\n",
        "\n",
        "im_list = natsorted(os.listdir('./testimages'))\n",
        "for f in im_list:\n",
        "  pth = './testimages/' + f \n",
        "  img = imageio.imread(pth)\n",
        "  if len(test_X) < testlimit:\n",
        "    test_X.append(np.array(img))\n",
        "test_X = np.array(test_X)\n",
        "test_X = test_X/255\n",
        "test_X = test_X[..., np.newaxis]\n",
        "\n",
        "im_list = natsorted(os.listdir('./testmasks'))\n",
        "for f in im_list:\n",
        "  pth = './testmasks/' + f \n",
        "  img = imageio.imread(pth)\n",
        "  if len(test_Y) < testlimit:\n",
        "    test_Y.append(np.array(img))\n",
        "test_Y = np.array(test_Y)\n",
        "test_Y = (test_Y > 0).astype(np.float32)\n",
        "test_Y = test_Y[..., np.newaxis]\n",
        "\n",
        "print(train_X.shape[0],\"images for training,\", val_X.shape[0], \"images for validation, and\", test_X.shape[0], \"images for testing\")\n",
        "\n",
        "WIDTH = train_X.shape[2]\n",
        "HEIGHT = train_X.shape[1]\n",
        "CHANNELS = 1\n",
        "print ('X and Y dims are ' + str(WIDTH) + 'x' + str(HEIGHT))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1679 images for training, 373 images for validation, and 297 images for testing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJu3FuYlbp-b",
        "colab_type": "text"
      },
      "source": [
        "#Cost Functions\n",
        "A critical element of every machine learning algorithm is to select a good cost function. In the classificaiton problems of prior articles, we used the accuracy of predictions as the cost function. For segmentation, one potentially could compute a score for teh classification of each pixel (object or not object), but a more common metric is the Dice Similiarity Score (a.k.a. Dice Score). This essentially means the amount of overlap between the gold standard and the prediction. If they perfectly agree, the Dice Score is 1, and if there is no overlap, the score is 0. We can convert the Dice Score toa Dice Loss by subtracting it from 1. One can also convert the Dice Score to a cross-entropy function, and this can work well in some cases. Code for both is provided, and you are encouraged to try both cost functions to see the effect. Note, however, that the above can result in Dice values of zero, and we don't want to divide by zero. In fact, we don't want to divide by a small number, so we add 'smooth' to the value (in our case we are using 1) so the values of Dice actually are 1 to 2, not 0 to 1. And that means the Dice Loss will be -1 to 0, but the loss reported is the sum, so it will have a larger range.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtR4fbhhhegw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dice_coeff(y_true, y_pred):\n",
        "    _epsilon = 10 ** -7\n",
        "    intersections = tf.reduce_sum(y_true * y_pred)\n",
        "    unions = tf.reduce_sum(y_true + y_pred)\n",
        "    dice_scores = (2.0 * intersections + _epsilon) / (unions + _epsilon)\n",
        "    return dice_scores\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    loss = 1 - dice_coeff(y_true, y_pred)\n",
        "    return loss\n",
        "  \n",
        "get_custom_objects().update({\"dice\": dice_loss})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDs_CyB-bxe6",
        "colab_type": "text"
      },
      "source": [
        "#Building the U-Net\n",
        "We are finally ready to build our Network. One difference of Keras versus the previous examples of FastAI is that Keras is designed so that there is one line of code for each layer of the network (plus the setup and training/evlauation code). This means that for a moderately complex network like a ResNet34 classifier, there would be 34 lines of code. U-Nets are popular network architectures for segmentation. These get their name because of the unique way they are built: The first part produces repeated reductions in resolution as the 'important' parts of the image are retained that reflect the structures that it is trained to recognize. Typically there are 4 or 5 such reduction layers that each consist of convultions, ReLUs, and Pooling layers. Once the image is reduced to the critical components, the networks begins to reconstruct the precise margins of the critical elements by using 'skip layers'. As the resolution is stored using convolutional-transpose layers, the layers 'look' back to the layers where the resolution was reduced to try to best define the margins of the structures. See Ronneberger (Ronneberger, Olaf; Fischer, Philipp; Brox, Thomas (2015). \"U-Net: Convolutional Networks for Biomedical Image Segmentation\". arXiv:1505.04597  https://arxiv.org/pdf/1505.04597.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-AbL6fvgPWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(act_fn = 'relu', init_fn = 'he_normal', width=256, height = 256, channels = 1, loss_fn = 'dice'):\n",
        "  \n",
        "    inputs = Input((width,height,channels))\n",
        "\n",
        "    conv1 = Conv2D(25, 3, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(inputs)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    conv2 = Conv2D(50, 3, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(pool1)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    conv3 = Conv2D(100, 3, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(pool2)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "    conv4 = Conv2D(200, 3, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(pool3)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "\n",
        "    conv5 = Conv2D(400, 3, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(pool4)\n",
        "\n",
        "    up6 = Conv2D(200, 2, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(UpSampling2D(size = (2,2))(conv5))\n",
        "    merge6 = concatenate([conv4,up6], axis = 3)\n",
        "    conv6 = Conv2D(200, 3, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(merge6)\n",
        "\n",
        "    up7 = Conv2D(100, 2, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(UpSampling2D(size = (2,2))(conv6))\n",
        "    merge7 = concatenate([conv3,up7], axis = 3)\n",
        "    conv7 = Conv2D(100, 3, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(merge7)\n",
        "\n",
        "    up8 = Conv2D(50, 2, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(UpSampling2D(size = (2,2))(conv7))\n",
        "    merge8 = concatenate([conv2,up8], axis = 3)\n",
        "    conv8 = Conv2D(50, 3, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(merge8)\n",
        "\n",
        "    up9 = Conv2D(25, 2, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(UpSampling2D(size = (2,2))(conv8))\n",
        "    merge9 = concatenate([conv1,up9], axis = 3)\n",
        "    conv9 = Conv2D(25, 3, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(merge9)\n",
        "    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
        "    model = Model(inputs = inputs, outputs = conv10)\n",
        "    model.summary()\n",
        "\n",
        "    if loss_fn == 'binary_cross_entropy':      # binary cross entropy\n",
        "        model.compile(optimizer = Adam(lr = 1e-4), loss = loss_fn, metrics=[accuracy])\n",
        "    else: \n",
        "        model.compile(optimizer = Adam(lr = 1e-4), loss = 'dice', metrics=[dice_coeff])\n",
        "    return model\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMReKXLxcCKc",
        "colab_type": "text"
      },
      "source": [
        "#Train it\n",
        "Now that our model is built, we are (finally!) ready to train it. Trial and error with colab hardware (K80) has shown that a batch_size of 16 works, and that we can get reasonable results adfter 5 epochs (but you can train longer to get better results)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMOBlqyRmA4w",
        "colab_type": "code",
        "outputId": "6be3df73-af51-4866-a0bd-d2cc43fbe0b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = build_model(act_fn = 'relu', init_fn = 'he_normal', width=WIDTH, height = HEIGHT, channels = CHANNELS, loss_fn = 'dice')\n",
        "\n",
        "checkpointer = ModelCheckpoint('model.h5', verbose=1, save_best_only=True)\n",
        "\n",
        "epochs = 25\n",
        "batch_size = 32\n",
        "results = model.fit(train_X, train_Y, validation_data=(val_X, val_Y), batch_size=batch_size, epochs=epochs, callbacks=[checkpointer])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 256, 256, 1)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 256, 256, 25) 250         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 128, 128, 25) 0           conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 128, 128, 50) 11300       max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, 64, 64, 50)   0           conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 64, 64, 100)  45100       max_pooling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2D)  (None, 32, 32, 100)  0           conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 32, 32, 200)  180200      max_pooling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2D)  (None, 16, 16, 200)  0           conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 16, 16, 400)  720400      max_pooling2d_8[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_5 (UpSampling2D)  (None, 32, 32, 400)  0           conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 32, 32, 200)  320200      up_sampling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 32, 32, 400)  0           conv2d_18[0][0]                  \n",
            "                                                                 conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 32, 32, 200)  720200      concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_6 (UpSampling2D)  (None, 64, 64, 200)  0           conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 64, 64, 100)  80100       up_sampling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 64, 64, 200)  0           conv2d_17[0][0]                  \n",
            "                                                                 conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 64, 64, 100)  180100      concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_7 (UpSampling2D)  (None, 128, 128, 100 0           conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 128, 128, 50) 20050       up_sampling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 128, 128, 100 0           conv2d_16[0][0]                  \n",
            "                                                                 conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 128, 128, 50) 45050       concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_8 (UpSampling2D)  (None, 256, 256, 50) 0           conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 256, 256, 25) 5025        up_sampling2d_8[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 256, 256, 50) 0           conv2d_15[0][0]                  \n",
            "                                                                 conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 256, 256, 25) 11275       concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 256, 256, 1)  26          conv2d_27[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 2,339,276\n",
            "Trainable params: 2,339,276\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Train on 1679 samples, validate on 373 samples\n",
            "Epoch 1/25\n",
            "1679/1679 [==============================] - 62s 37ms/step - loss: 0.8979 - dice_coeff: 0.1021 - val_loss: 0.9077 - val_dice_coeff: 0.0923\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.90767, saving model to model.h5\n",
            "Epoch 2/25\n",
            "1679/1679 [==============================] - 55s 32ms/step - loss: 0.7482 - dice_coeff: 0.2518 - val_loss: 0.7758 - val_dice_coeff: 0.2242\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.90767 to 0.77576, saving model to model.h5\n",
            "Epoch 3/25\n",
            "1679/1679 [==============================] - 54s 32ms/step - loss: 0.6306 - dice_coeff: 0.3694 - val_loss: 0.5859 - val_dice_coeff: 0.4141\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.77576 to 0.58586, saving model to model.h5\n",
            "Epoch 4/25\n",
            "1679/1679 [==============================] - 54s 32ms/step - loss: 0.5707 - dice_coeff: 0.4293 - val_loss: 0.5402 - val_dice_coeff: 0.4598\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.58586 to 0.54024, saving model to model.h5\n",
            "Epoch 5/25\n",
            "1679/1679 [==============================] - 54s 32ms/step - loss: 0.5139 - dice_coeff: 0.4861 - val_loss: 0.5410 - val_dice_coeff: 0.4590\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.54024\n",
            "Epoch 6/25\n",
            "1679/1679 [==============================] - 54s 32ms/step - loss: 0.4864 - dice_coeff: 0.5136 - val_loss: 0.4734 - val_dice_coeff: 0.5266\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.54024 to 0.47344, saving model to model.h5\n",
            "Epoch 7/25\n",
            "1679/1679 [==============================] - 54s 32ms/step - loss: 0.4238 - dice_coeff: 0.5762 - val_loss: 0.4945 - val_dice_coeff: 0.5055\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.47344\n",
            "Epoch 8/25\n",
            "1679/1679 [==============================] - 54s 32ms/step - loss: 0.4151 - dice_coeff: 0.5849 - val_loss: 0.4624 - val_dice_coeff: 0.5376\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.47344 to 0.46244, saving model to model.h5\n",
            "Epoch 9/25\n",
            "1679/1679 [==============================] - 54s 32ms/step - loss: 0.3836 - dice_coeff: 0.6164 - val_loss: 0.4752 - val_dice_coeff: 0.5248\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.46244\n",
            "Epoch 10/25\n",
            "1679/1679 [==============================] - 54s 32ms/step - loss: 0.3461 - dice_coeff: 0.6539 - val_loss: 0.4421 - val_dice_coeff: 0.5579\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.46244 to 0.44209, saving model to model.h5\n",
            "Epoch 11/25\n",
            "1679/1679 [==============================] - 54s 32ms/step - loss: 0.3453 - dice_coeff: 0.6547 - val_loss: 0.4245 - val_dice_coeff: 0.5755\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.44209 to 0.42449, saving model to model.h5\n",
            "Epoch 12/25\n",
            "1679/1679 [==============================] - 54s 32ms/step - loss: 0.3189 - dice_coeff: 0.6811 - val_loss: 0.3955 - val_dice_coeff: 0.6045\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.42449 to 0.39548, saving model to model.h5\n",
            "Epoch 13/25\n",
            "1679/1679 [==============================] - 54s 32ms/step - loss: 0.2889 - dice_coeff: 0.7111 - val_loss: 0.3878 - val_dice_coeff: 0.6122\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.39548 to 0.38783, saving model to model.h5\n",
            "Epoch 14/25\n",
            "1679/1679 [==============================] - 54s 32ms/step - loss: 0.2833 - dice_coeff: 0.7167 - val_loss: 0.3894 - val_dice_coeff: 0.6106\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.38783\n",
            "Epoch 15/25\n",
            "1679/1679 [==============================] - 54s 32ms/step - loss: 0.2608 - dice_coeff: 0.7392 - val_loss: 0.3734 - val_dice_coeff: 0.6266\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.38783 to 0.37343, saving model to model.h5\n",
            "Epoch 16/25\n",
            "1679/1679 [==============================] - 54s 32ms/step - loss: 0.2515 - dice_coeff: 0.7485 - val_loss: 0.3801 - val_dice_coeff: 0.6199\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.37343\n",
            "Epoch 17/25\n",
            "1679/1679 [==============================] - 54s 32ms/step - loss: 0.2299 - dice_coeff: 0.7701 - val_loss: 0.4058 - val_dice_coeff: 0.5942\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.37343\n",
            "Epoch 18/25\n",
            "1679/1679 [==============================] - 54s 32ms/step - loss: 0.2253 - dice_coeff: 0.7747 - val_loss: 0.3604 - val_dice_coeff: 0.6396\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.37343 to 0.36042, saving model to model.h5\n",
            "Epoch 19/25\n",
            " 960/1679 [================>.............] - ETA: 21s - loss: 0.2193 - dice_coeff: 0.7807"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXfBIVWNcJit",
        "colab_type": "text"
      },
      "source": [
        "#Test it!\n",
        "Now that the model is trained, we can test our segmentation tool on the test holdout cases. Note that the prediction is the probability that a pixel is object (pancreas) or not object (anything other than pancreas). We use 0.5 as the threshold for deciding Pancreas or not, though again you can adjust this if your task might view it as more valuable to include non-pancreas pixels in order to reduce the number of missed pancreas pixels.\n",
        "Note also that we calculate the Dice Score. The Dice calculated in training is not the actual Dice score, but it is proportional (we make adjustments so it is never 0 or have a divide by 0 error), and therefore works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iV6XJMaLmeA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights(\"./model.h5\")\n",
        "preds_test = model.predict(test_X, verbose=1)\n",
        "preds_test = (preds_test > 0.5).astype(np.uint8)\n",
        "\n",
        "def np_dice(true, pred):\n",
        "    intersection = np.sum(true * pred)\n",
        "    dc =(2.0 * intersection) / (np.sum(true) + np.sum(pred))\n",
        "    return dc\n",
        "\n",
        "fig=plt.figure(figsize=(130, 130), dpi = 75)\n",
        "\n",
        "for j in range(0,8,2):\n",
        "    i = random.randint(0,test_X.shape[0]-1)\n",
        "    image = test_X[i,...,0]\n",
        "    mask =  test_Y[i,...,0]\n",
        "    mask = ma.masked_where(mask == 0, mask)\n",
        "    pred = preds_test[i,...,0]\n",
        "    pred = ma.masked_where(pred == 0, pred)\n",
        "    \n",
        "    fig.add_subplot(8, 2, j+1)\n",
        "    plt.imshow(image, cmap = \"gray\")\n",
        "    plt.imshow(mask, 'cool', alpha=0.7)\n",
        "    plt.title(\"Ground Truth\")\n",
        "    plt.axis('off')\n",
        "    \n",
        "    fig.add_subplot(8, 2, j+2)\n",
        "    plt.imshow(image, cmap = \"gray\")\n",
        "    plt.imshow(pred, 'cool', alpha=0.7)\n",
        "    plt.title(\"Prediction\")\n",
        "    plt.axis('off')\n",
        "    \n",
        "plt.subplots_adjust(bottom=0.1, left = 0.01, right=0.05, top=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(\"The dice score for this model is: \", np_dice(test_Y, preds_test))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}